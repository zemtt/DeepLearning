{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 《神经网络与深度学习》学习笔记\n",
    "## 第5章 卷积神经网络\n",
    "* 卷积神经网络是一种具有**局部连接**、**权重贡献**、**汇聚**等特性的深层前馈神经网络。\n",
    "* 全连接神经网络的问题：\n",
    "  * 参数太多。\n",
    "  * 局部不变性特征，自然图像中的物体都具有不变性特征，如缩放、平移、旋转等操作不影响其语义。全连接前馈神经网络很难提取这些局部不变的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1 卷积\n",
    "* 一维卷积：\n",
    "  * $ y_{t} = \\sum_{k=1}^{m}w_{k}\\cdot x_{t-k+1} $\n",
    "  * $ \\mathbf{y}=\\mathbf{w} \\otimes \\mathbf{x} $\n",
    "  * $\\mathbf{x}$为输入序列，$\\mathbf{w}$为卷积核，$\\mathbf{y}$为输出序列。\n",
    "* 二维卷积(具体可视化内容参考教材)：\n",
    "  * $ y_{ij} = \\sum_{u=1}^{m}\\sum_{v=1}^{n}w_{uv}\\cdot x_{i-u+1, j-v+1} $\n",
    "\n",
    "#### 5.1.1 互相关\n",
    "* 卷积核互相关的区别仅在于卷积核是否进行翻转，因为为了描述方便，用互相关来代替卷积。\n",
    "\n",
    "#### 5.1.2 卷积的变种\n",
    "* 步长、零填充\n",
    "* 窄卷积、宽卷积、等宽卷积\n",
    "\n",
    "#### 5.1.3 卷积的数学性质\n",
    "* 交换性：$\\mathbf{x}\\otimes\\mathbf{y}=\\mathbf{y}\\otimes\\mathbf{x}$\n",
    "* 导数：假设：$Y=\\mathbf{W}\\otimes\\mathbf{X}$，且$f(Y)$是一个标量函数，则$\\frac{\\partial f(Y)}{\\partial W}=\\frac{\\partial f(Y)}{\\partial Y} \\otimes X$"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.2 卷积神经网络\n",
    "* 卷积神经网络一般由卷积层、汇聚层和全连接层构成。\n",
    "\n",
    "#### 5.2.1 用卷积来代替全连接\n",
    "* 用卷积代替全连接后，$\\mathbf{z}^{(l)} = \\mathbf{w}^{(l)}\\otimes\\mathbf{a}^{(l-1)}+b^{(l)}$\n",
    "\n",
    "#### 5.2.2 卷积层\n",
    "* 在图片处理的过程中，神经元的组织通常为三维结构，大小为高度$ M \\times$宽度$N \\times$深度$D$，由$D$个$M\\times N$大小的特征映射构成。\n",
    "* 卷积层结构：\n",
    "  * 输入：三维张量，$\\mathbf{X} \\in \\mathbb{R}^{M\\times N\\times D}$，其中$X^{d}\\in \\mathbb{R}^{M\\times N}$。\n",
    "  * 输出：三维张量，$\\mathbf{Y} \\in \\mathbb{R}^{M'\\times N'\\times P}$，其中$Y^{p}\\in \\mathbb{R}^{M'\\times N'}$。\n",
    "  * 卷积核：四维张量，$\\mathbf{W}\\in\\mathbb{R}^{m\\times n\\times D\\times P}$，其中$W^{p,d}\\in\\mathbb{R}^{m\\times n}$是一个二维卷积核。\n",
    "\n",
    "#### 5.2.3 汇聚层\n",
    "* 汇聚层又叫子采样层，其作用是进行特征选择，降低特征数量，并从而减少参数数量。\n",
    "* 常见的汇聚函数：最大汇聚、平均汇聚。\n",
    "\n",
    "#### 典型的卷积神经网络\n",
    "* 不方便描述，参考教材。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.3 参数学习\n",
    "* 同样采用反向传播算法，只不过为卷积操作做了一些小修改。\n",
    "\n",
    "#### 5.3.1 误差项的计算\n",
    "* 汇聚层：\n",
    "  * 因为汇聚层采用了下采样操作，所以误差项传递进行上采样操作。\n",
    "  * 对于最大汇聚，误差项只传递到上一层对应区域中最大值所对应的神经元，平均汇聚则平均分配到上一层。\n",
    "* 卷积层：\n",
    "  * 根据5.1.3中的求导法则传递误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.4 几种典型的卷积神经网络\n",
    "* LeNet-5、AlexNet、Inception网络、残差网络。\n",
    "  * 残差网络：将函数拆分为线性恒等函数和残差函数，残差函数用等宽卷积函数去逼近。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.5 其他卷积方式\n",
    "* 转置卷积\n",
    "* 微步卷积\n",
    "* 空洞卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}