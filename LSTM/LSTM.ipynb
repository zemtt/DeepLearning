{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36264bitpy3condab5e638c816e74e20902219e2b7f9cf05",
   "display_name": "Python 3.6.2 64-bit ('py3': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.MyData import MyData\n",
    "from utils.tools import list_to_one_hot\n",
    "from torch import nn, Tensor\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 1e-1\n",
    "num_epochs = 10\n",
    "\n",
    "class_num = 21284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MyData('./wbqg3541/train.csv')\n",
    "train_data = data.train_data()\n",
    "test_data = data.test_data()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sentence_matrix = [list_to_one_hot(item[0]) for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    labels = torch.Tensor(labels).long()\n",
    "    return [sentence_matrix,labels]\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True,collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性模型实现\n",
    "class LinerClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinerClassifier, self).__init__()\n",
    "        self.L1 = nn.Sequential(\n",
    "            nn.Linear(21284,20),\n",
    "            nn.ReLU(True))\n",
    "        self.L2 = nn.Sequential(\n",
    "            nn.Linear(20,2),\n",
    "            nn.LogSoftmax())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 这里的x是一个长度为batchsize的list，\n",
    "        # list中的每一个元素是class_num * sentence_len的one-hot矩阵\n",
    "        # 因为这是一个线性模型，不是一个不定长序列模型\n",
    "        # 我们将矩阵压缩到class_num * 1的向量sentence_v\n",
    "        # 其中i元素的值代表第i个单词在句子中出现的次数\n",
    "        # 将batch中的sentence_v组成一个新的矩阵sum_matrix作为新的输入\n",
    "        sum_matrix = Tensor([])\n",
    "        for each in x:\n",
    "            sum_matrix = torch.cat((sum_matrix, each.sum(1).reshape(1, class_num)), 0)\n",
    "        x = self.L1(sum_matrix)\n",
    "        x = self.L2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM模型实现\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.L1 = nn.LSTM(\n",
    "            input_size=class_num,\n",
    "            hidden_size=16,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bias=False\n",
    "        )\n",
    "        self.L2 = nn.Sequential(\n",
    "            nn.Linear(16 ,2),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = [each.t() for each in x]\n",
    "        x = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "        output,(h_n,c_n) = self.L1(x)\n",
    "        # print(output.size())\n",
    "        output_in_last_timestep=h_n[-1,:,:]\n",
    "        x = self.L2(output_in_last_timestep)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence_matrixs, label in train_loader:\n",
    "    sentence_matrixs = [each.t() for each in sentence_matrixs]\n",
    "    sentence_matrixs = pad_sequence(sentence_matrixs, batch_first=True, padding_value=0)\n",
    "    print(sentence_matrixs.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier()\n",
    "\n",
    "for sentence_matrixs, label in train_loader:\n",
    "    print(model(sentence_matrixs).size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('*' * 10)\n",
    "    print(f'epoch {epoch+1}')\n",
    "    running_loss, running_acc = .0, .0\n",
    "    for sentence_matrixs, label in train_loader:\n",
    "        out = model(sentence_matrixs)\n",
    "        loss = criterion(out, label)\n",
    "        running_loss += loss.item()\n",
    "        _, pred = torch.max(out, 1)\n",
    "        running_acc += (pred == label).float().mean()\n",
    "        # 向后传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Train Loss: {running_loss/len(train_loader):.6f}, Acc: {running_acc/len(train_loader):.6f}')\n",
    "    model.eval()\n",
    "    eval_loss, eval_acc = .0, .0\n",
    "    for sentence_matrixs, label in test_loader:\n",
    "        with torch.no_grad():\n",
    "            out = model(sentence_matrixs)\n",
    "            loss = criterion(out, label)\n",
    "        eval_loss += loss.item()\n",
    "        _, pred = torch.max(out, 1)\n",
    "        eval_acc += (pred == label).float().mean()\n",
    "    acc.append(eval_acc/len(test_loader))\n",
    "    print(f'Test Loss: {eval_loss/len(test_loader):.6f}, Acc: {eval_acc/len(test_loader):.6f}\\n')\n",
    "    \n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), './ReNet-5.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}